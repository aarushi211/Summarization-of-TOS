# üìú TOS-Summarizer: Distilled Legal AI

![Python](https://img.shields.io/badge/Python-3.9-blue)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-009688)
![LlamaCPP](https://img.shields.io/badge/Llama_CPP-Quantized-orange)
![FAISS](https://img.shields.io/badge/Vector_DB-FAISS-yellow)
![LangChain](https://img.shields.io/badge/Orchestration-LangChain-green)

## üöÄ Project Overview

Terms of Service (TOS) agreements are notoriously long and complex. This project creates a privacy-centric, efficient AI assistant capable of generating executive summaries and answering specific legal questions about TOS documents.

Unlike generic LLM wrappers, this project focuses on Knowledge Distillation and Edge Deployment. It uses a small, specialized model (Qwen 1.5B) trained on synthetic data generated by a massive teacher model (Llama 3.1 8B), allowing it to run efficiently on low-resource hardware (CPU/Free Tier Cloud).

## üèóÔ∏è Architecture

The system follows a Hybrid RAG (Retrieval-Augmented Generation) architecture to balance global context understanding with specific fact retrieval.

graph TD
```
    A[User Uploads PDF] --> B{Input Handler}
    B -->|Global Summary| C[Fine-Tuned Qwen 1.5B]
    B -->|Specific Question| D[RAG Pipeline]
    D --> E[FAISS Vector Store]
    E --> F[Retrieved Context]
    F --> C
    C --> G[Final Response]
```


- Frontend: Streamlit (User Interface).
- Backend: FastAPI (Model Inference Microservice).
- Model: Qwen 2.5-1.5B-Instruct (Fine-tuned via QLoRA).
- Infrastructure: Dockerized and deployable to Google Cloud Run (Scale-to-Zero).

## üß† Methodology & Engineering Decisions

### Synthetic Data Pipeline (The "Teacher-Student" Loop)

Lack of high-quality abstractive summarization datasets for TOS led to a synthetic data strategy:

- Teacher Model: Llama 3.1 8B (via vLLM for high-throughput inference).
- Process: Generated 9,000+ abstractive summaries from raw TOS text files.
- Optimization: Used 4-bit quantization (AWQ) to run the teacher model on a single T4 GPU during generation.

### Fine-Tuning (Knowledge Distillation)

Instead of deploying the massive Llama 3.1 model, I distilled its knowledge into Qwen 1.5B.

- Technique: QLoRA (Quantized Low-Rank Adaptation).
- Hyperparameters: Optimized for T4 GPU constraints (Batch size 1, Gradient Accumulation 8).
- Result: A 3GB model that achieves ROUGE-1: ~0.40, matching the performance of larger models while being 80% cheaper to run.

### Hybrid Inference

Standard RAG fails at "Summarize the whole document" because it only retrieves chunks.

- Strategy: I implemented a hybrid router.
- Summarization: Feeds the truncated full document (up to 4k tokens) to the fine-tuned model.
- Q&A: Uses FAISS and LangChain to retrieve specific clauses for precision.

## üõ†Ô∏è Tech Stack

- Model Training: transformers, peft, trl, bitsandbytes
- Data Processing: vLLM, pandas, tqdm
- Inference: onnxruntime / torch (CPU Optimized)
- Deployment: FastAPI, Docker, Google Cloud Run
- UI: Streamlit

## üìä Performance Metrics

Evaluated on a held-out validation set of 100 documents:


## üíª Installation & Usage

### 1. Local Setup

#### Clone repo
```
git clone [https://github.com/yourusername/Summarization-of-TOS.git](https://github.com/yourusername/Summarization-of-TOS.git)
cd Summarization-of-TOS
```

#### Install dependencies using uv (fast pip alternative)
```
pip install uv
uv pip install -r requirements.txt
```

#### Run the App
```
uv run streamlit run app/app.py
```

### 2. Docker Build
```
docker build -f api/Dockerfile -t tos-api .
docker run -p 8080:8080 tos-api
```