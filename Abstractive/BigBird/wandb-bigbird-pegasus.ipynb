{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.30\n!pip install accelerate -U\n!pip install sentencepiece\n!pip install rouge\n!pip install wandb onnx -Uq","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-OBrGewb_Wu","outputId":"fb6dde92-42df-4237-f955-1f551db4c196","execution":{"iopub.status.busy":"2023-10-26T12:19:03.205252Z","iopub.execute_input":"2023-10-26T12:19:03.205554Z","iopub.status.idle":"2023-10-26T12:20:03.482804Z","shell.execute_reply.started":"2023-10-26T12:19:03.205513Z","shell.execute_reply":"2023-10-26T12:20:03.481734Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.30 in /opt/conda/lib/python3.10/site-packages (4.30.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (2023.7.22)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.24.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.16.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nRequirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (1.0.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/Arjavjain100/TOS-Summarization.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-7bYF_qvTDT","outputId":"67907995-ed97-4365-8f6d-31f8f706ce20","execution":{"iopub.status.busy":"2023-10-26T12:20:03.484381Z","iopub.execute_input":"2023-10-26T12:20:03.485241Z","iopub.status.idle":"2023-10-26T12:20:04.446470Z","shell.execute_reply.started":"2023-10-26T12:20:03.485202Z","shell.execute_reply":"2023-10-26T12:20:04.445452Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"fatal: destination path 'TOS-Summarization' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BigBirdPegasusForConditionalGeneration, Trainer, TrainingArguments,pipeline,PretrainedConfig, BigBirdPegasusConfig\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sklearn.model_selection import train_test_split\nfrom rouge import Rouge\nimport pandas as pd\nimport os\nimport wandb\nimport random\nimport numpy as np\nimport accelerate\n\nos.environ[\"WANDB_PROJECT\"]=\"major-one\"\nos.environ[\"WANDB_LOG_MODEL\"]=\"checkpoint\"\nos.environ[\"WANDB_WATCH\"]=\"all\"\n\n\n\n# Ensure deterministic behavior\ntorch.backends.cudnn.deterministic = True\nrandom.seed(hash(\"setting random seeds\") % 2**32 - 1)\nnp.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\ntorch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\ntorch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n\n# Device configuration\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset location\nfilename = \"./TOS-Summarization/Dataset/all_v1_transpose.csv\"","metadata":{"id":"7KBtpTeJbcsA","execution":{"iopub.status.busy":"2023-10-26T12:20:04.449804Z","iopub.execute_input":"2023-10-26T12:20:04.450208Z","iopub.status.idle":"2023-10-26T12:20:11.665928Z","shell.execute_reply.started":"2023-10-26T12:20:04.450168Z","shell.execute_reply":"2023-10-26T12:20:11.665106Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb -U","metadata":{"execution":{"iopub.status.busy":"2023-10-26T12:20:11.667039Z","iopub.execute_input":"2023-10-26T12:20:11.667406Z","iopub.status.idle":"2023-10-26T12:20:23.804893Z","shell.execute_reply.started":"2023-10-26T12:20:11.667373Z","shell.execute_reply":"2023-10-26T12:20:23.803606Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.12)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.login()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NquUD6lhfBHO","outputId":"47d4f550-2eb9-4b7a-b273-73ad2c3800d0","execution":{"iopub.status.busy":"2023-10-26T12:20:23.806571Z","iopub.execute_input":"2023-10-26T12:20:23.806915Z","iopub.status.idle":"2023-10-26T12:20:25.775643Z","shell.execute_reply.started":"2023-10-26T12:20:23.806888Z","shell.execute_reply":"2023-10-26T12:20:25.774621Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maarushi-jain211\u001b[0m (\u001b[33mfaltu-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(filename)\ndf = df[['original_text','reference_summary']]\ndf.rename(columns = {'original_text':'source', 'reference_summary':'target'}, inplace = True)\nlen(df)","metadata":{"id":"ERL_fIRDcBOB","execution":{"iopub.status.busy":"2023-10-26T12:20:25.776996Z","iopub.execute_input":"2023-10-26T12:20:25.778249Z","iopub.status.idle":"2023-10-26T12:20:25.801484Z","shell.execute_reply.started":"2023-10-26T12:20:25.778205Z","shell.execute_reply":"2023-10-26T12:20:25.800332Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"446"},"metadata":{}}]},{"cell_type":"code","source":"X = df['source']\ny = df['target']","metadata":{"id":"RnCjv6n7wVrN","execution":{"iopub.status.busy":"2023-10-26T12:20:25.802805Z","iopub.execute_input":"2023-10-26T12:20:25.803087Z","iopub.status.idle":"2023-10-26T12:20:25.807490Z","shell.execute_reply.started":"2023-10-26T12:20:25.803063Z","shell.execute_reply":"2023-10-26T12:20:25.806572Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"K35erKW3cINP","execution":{"iopub.status.busy":"2023-10-26T12:20:25.808949Z","iopub.execute_input":"2023-10-26T12:20:25.809596Z","iopub.status.idle":"2023-10-26T12:20:25.826007Z","shell.execute_reply.started":"2023-10-26T12:20:25.809562Z","shell.execute_reply":"2023-10-26T12:20:25.825122Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                              source  \\\n0  welcome to the pokémon go video game services ...   \n1  by using our services you are agreeing to thes...   \n2  if you want to use certain features of the ser...   \n3  during game play please be aware of your surro...   \n4  subject to your compliance with these terms ni...   \n\n                                              target  \n0                                                hi.  \n1  by playing this game you agree to these terms....  \n2  you have to use google pokemon trainer club or...  \n3  don t die or hurt others and if you do it s no...  \n4  don t copy modify resell distribute or reverse...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>welcome to the pokémon go video game services ...</td>\n      <td>hi.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>by using our services you are agreeing to thes...</td>\n      <td>by playing this game you agree to these terms....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>if you want to use certain features of the ser...</td>\n      <td>you have to use google pokemon trainer club or...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>during game play please be aware of your surro...</td>\n      <td>don t die or hurt others and if you do it s no...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>subject to your compliance with these terms ni...</td>\n      <td>don t copy modify resell distribute or reverse...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class BigBirdDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels['input_ids'])","metadata":{"id":"YuCMAtjGeM86","execution":{"iopub.status.busy":"2023-10-26T12:20:25.831062Z","iopub.execute_input":"2023-10-26T12:20:25.831341Z","iopub.status.idle":"2023-10-26T12:20:25.837804Z","shell.execute_reply.started":"2023-10-26T12:20:25.831316Z","shell.execute_reply":"2023-10-26T12:20:25.836920Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def prepare_data(model_name,\n                 train_texts, train_labels,\n                 test_texts, test_labels):\n    \"\"\"\n    Prepare input data for model fine-tuning\n    \"\"\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    prepare_test = False if test_texts is None or test_labels is None else True\n\n    def tokenize_data(texts, labels):\n\n        encodings = tokenizer(texts, truncation=True, padding=True, max_length = 600)\n        decodings = tokenizer(labels, truncation=True, padding=True, max_length = 256)\n        dataset_tokenized = BigBirdDataset(encodings, decodings)\n        return dataset_tokenized\n\n    train_dataset = tokenize_data(train_texts, train_labels)\n    test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n\n    return train_dataset, test_dataset, tokenizer","metadata":{"id":"NOdzySbseTAg","execution":{"iopub.status.busy":"2023-10-26T12:20:25.840227Z","iopub.execute_input":"2023-10-26T12:20:25.840610Z","iopub.status.idle":"2023-10-26T12:20:25.849785Z","shell.execute_reply.started":"2023-10-26T12:20:25.840575Z","shell.execute_reply":"2023-10-26T12:20:25.848715Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def prepare_fine_tuning(model_name, tokenizer, train_dataset, test_dataset, freeze_encoder=False, output_dir='./results'):\n    \"\"\"\n    Prepare configurations and base model for fine-tuning\n    \"\"\"\n    torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    config = BigBirdPegasusConfig.from_pretrained(model_name, attention_type='block_sparse', num_random_blocks=3, block_size=32, truncate = True)\n    model = BigBirdPegasusForConditionalGeneration.from_pretrained(model_name, config = config).to(torch_device)\n\n    if test_dataset is not None:\n        training_args = TrainingArguments(\n          output_dir=output_dir,           # output directory\n          num_train_epochs=2,              # total number of training epochs\n          per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n          per_device_eval_batch_size=1,    # batch size for evaluation, can increase if memory allows\n          save_steps=500,                  # number of updates steps before checkpoint saves\n          save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n          evaluation_strategy='steps',     # evaluation strategy to adopt during training\n          eval_steps=100,                  # number of update steps before evaluation\n          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n          weight_decay=0.01,               # strength of weight decay\n          logging_dir='./logs',            # directory for storing logs\n          logging_steps=100,\n          report_to=\"wandb\",\n          run_name = 'bigbird'\n        )\n\n        trainer = Trainer(\n          model=model,                         # the instantiated 🤗 Transformers model to be trained\n          args=training_args,                  # training arguments, defined above\n          train_dataset=train_dataset,         # training dataset\n          eval_dataset=test_dataset,           # evaluation dataset\n          tokenizer=tokenizer\n        )\n\n    else:\n        training_args = TrainingArguments(\n          output_dir=output_dir,           # output directory\n          num_train_epochs=2,              # total number of training epochs\n          per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n          save_steps=500,                  # number of updates steps before checkpoint saves\n          save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n          weight_decay=0.01,               # strength of weight decay\n          logging_dir='./logs',            # directory for storing logs\n          logging_steps=100,\n        )\n\n        trainer = Trainer(\n          model=model,                         # the instantiated 🤗 Transformers model to be trained\n          args=training_args,                  # training arguments, defined above\n          train_dataset=train_dataset,         # training dataset\n          tokenizer=tokenizer\n        )\n\n    return trainer","metadata":{"id":"8sFuuoDheVvr","execution":{"iopub.status.busy":"2023-10-26T12:20:25.851367Z","iopub.execute_input":"2023-10-26T12:20:25.851977Z","iopub.status.idle":"2023-10-26T12:20:25.863273Z","shell.execute_reply.started":"2023-10-26T12:20:25.851944Z","shell.execute_reply":"2023-10-26T12:20:25.862554Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\ntrain_texts, train_labels = list(X_train), list(y_train)\ntest_texts, test_labels = list(X_test), list(y_test)","metadata":{"id":"1Qs6bhoOz8uV","execution":{"iopub.status.busy":"2023-10-26T12:20:25.864306Z","iopub.execute_input":"2023-10-26T12:20:25.864607Z","iopub.status.idle":"2023-10-26T12:20:25.881338Z","shell.execute_reply.started":"2023-10-26T12:20:25.864581Z","shell.execute_reply":"2023-10-26T12:20:25.880325Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model_name = 'google/bigbird-pegasus-large-arxiv'\n\ntrain_dataset,test_dataset, tokenizer = prepare_data(model_name, train_texts, train_labels,test_texts,test_labels)\ntrainer = prepare_fine_tuning(model_name, tokenizer, train_dataset,test_dataset)\n\ntrainer.train()\n\ntrainer.evaluate(test_dataset)\n\nwandb.finish()","metadata":{"id":"PBpl_Dzoebmd","execution":{"iopub.status.busy":"2023-10-26T12:20:25.882351Z","iopub.execute_input":"2023-10-26T12:20:25.882683Z","iopub.status.idle":"2023-10-26T12:36:51.713428Z","shell.execute_reply.started":"2023-10-26T12:20:25.882639Z","shell.execute_reply":"2023-10-26T12:36:51.712351Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231026_122039-0etl861k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/faltu-team/major-one/runs/0etl861k' target=\"_blank\">bigbird</a></strong> to <a href='https://wandb.ai/faltu-team/major-one' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/faltu-team/major-one' target=\"_blank\">https://wandb.ai/faltu-team/major-one</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/faltu-team/major-one/runs/0etl861k' target=\"_blank\">https://wandb.ai/faltu-team/major-one/runs/0etl861k</a>"},"metadata":{}},{"name":"stderr","text":"You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='356' max='356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [356/356 14:53, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>10.495200</td>\n      <td>8.897735</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>5.209300</td>\n      <td>0.914791</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.973000</td>\n      <td>0.597813</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [45/45 00:34]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='2209.304 MB of 2209.304 MB uploaded (8.107 MB deduped)\\r'), FloatProgress(value=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb4da8f3ad8401f831cb4628a630379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁█</td></tr><tr><td>eval/samples_per_second</td><td>███▁</td></tr><tr><td>eval/steps_per_second</td><td>███▁</td></tr><tr><td>train/epoch</td><td>▁▁▄▄▆▆██</td></tr><tr><td>train/global_step</td><td>▁▁▄▄▆▆██</td></tr><tr><td>train/learning_rate</td><td>▁▅█</td></tr><tr><td>train/loss</td><td>█▄▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.55002</td></tr><tr><td>eval/runtime</td><td>34.4899</td></tr><tr><td>eval/samples_per_second</td><td>2.609</td></tr><tr><td>eval/steps_per_second</td><td>1.305</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>356</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.973</td></tr><tr><td>train/total_flos</td><td>1204944489676800.0</td></tr><tr><td>train/train_loss</td><td>4.81423</td></tr><tr><td>train/train_runtime</td><td>853.9067</td></tr><tr><td>train/train_samples_per_second</td><td>0.834</td></tr><tr><td>train/train_steps_per_second</td><td>0.417</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bigbird</strong> at: <a href='https://wandb.ai/faltu-team/major-one/runs/0etl861k' target=\"_blank\">https://wandb.ai/faltu-team/major-one/runs/0etl861k</a><br/>Synced 6 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231026_122039-0etl861k/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nif not os.path.exists('./ouput_model/'):\n    os.makedirs('./ouput_model/')\ntrainer.model.save_pretrained(\"./ouput_model/\")","metadata":{"id":"IlT-HJrKefPH","execution":{"iopub.status.busy":"2023-10-26T12:36:51.717186Z","iopub.execute_input":"2023-10-26T12:36:51.717599Z","iopub.status.idle":"2023-10-26T12:36:55.153574Z","shell.execute_reply.started":"2023-10-26T12:36:51.717558Z","shell.execute_reply":"2023-10-26T12:36:55.152595Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Inference","metadata":{"id":"WtBBBTxTuwmy"}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"id":"S6RupAQGv2G4","execution":{"iopub.status.busy":"2023-10-26T12:36:55.154930Z","iopub.execute_input":"2023-10-26T12:36:55.155253Z","iopub.status.idle":"2023-10-26T12:37:02.795098Z","shell.execute_reply.started":"2023-10-26T12:36:55.155222Z","shell.execute_reply":"2023-10-26T12:37:02.793853Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"config = PretrainedConfig.from_json_file('./ouput_model/config.json')","metadata":{"id":"Vy4oVHjuwA5k","execution":{"iopub.status.busy":"2023-10-26T12:37:02.796699Z","iopub.execute_input":"2023-10-26T12:37:02.797929Z","iopub.status.idle":"2023-10-26T12:37:03.032076Z","shell.execute_reply.started":"2023-10-26T12:37:02.797882Z","shell.execute_reply":"2023-10-26T12:37:03.030625Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"./ouput_model/\").to(device)","metadata":{"id":"wjV9aYqwvsDL","execution":{"iopub.status.busy":"2023-10-26T12:37:03.034153Z","iopub.execute_input":"2023-10-26T12:37:03.034575Z","iopub.status.idle":"2023-10-26T12:37:13.748194Z","shell.execute_reply.started":"2023-10-26T12:37:03.034516Z","shell.execute_reply":"2023-10-26T12:37:13.747265Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def summarize(text):\n  input_tokenized = tokenizer.encode(text, return_tensors='pt',max_length=1024,truncation=True).to(device)\n  summary_ids = model.generate(input_tokenized,\n                                  num_beams=9,\n                                  no_repeat_ngram_size=3,\n                                  length_penalty=2.0,\n                                  min_length=50,\n                                  max_length=150,\n                                  early_stopping=True)\n  summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n\n  return summary","metadata":{"id":"4eFewXp-vc6F","execution":{"iopub.status.busy":"2023-10-26T12:37:13.749483Z","iopub.execute_input":"2023-10-26T12:37:13.750311Z","iopub.status.idle":"2023-10-26T12:37:13.757032Z","shell.execute_reply.started":"2023-10-26T12:37:13.750273Z","shell.execute_reply":"2023-10-26T12:37:13.755964Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"y_pred = X_test.apply(lambda x: summarize(x))","metadata":{"id":"-W86Su68vqFv","execution":{"iopub.status.busy":"2023-10-26T12:37:13.758460Z","iopub.execute_input":"2023-10-26T12:37:13.758863Z","iopub.status.idle":"2023-10-26T12:39:31.631282Z","shell.execute_reply.started":"2023-10-26T12:37:13.758828Z","shell.execute_reply":"2023-10-26T12:39:31.630249Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Attention type 'block_sparse' is not possible if sequence_length: 78 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 352 with config.block_size = 32, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n","output_type":"stream"}]},{"cell_type":"code","source":"summary = pd.concat([y_test.to_frame(name=\"reference_summary\"), y_pred.to_frame(name=\"generated_summary\")], axis=1)","metadata":{"id":"Jj6hYeguyHsd","execution":{"iopub.status.busy":"2023-10-26T12:39:31.632768Z","iopub.execute_input":"2023-10-26T12:39:31.633135Z","iopub.status.idle":"2023-10-26T12:39:31.639777Z","shell.execute_reply.started":"2023-10-26T12:39:31.633102Z","shell.execute_reply":"2023-10-26T12:39:31.638839Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"rouge = Rouge()","metadata":{"id":"QbsGPBCqyZoH","execution":{"iopub.status.busy":"2023-10-26T12:39:31.641049Z","iopub.execute_input":"2023-10-26T12:39:31.641379Z","iopub.status.idle":"2023-10-26T12:39:31.654591Z","shell.execute_reply.started":"2023-10-26T12:39:31.641347Z","shell.execute_reply":"2023-10-26T12:39:31.653724Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"rouge.get_scores(summary['generated_summary'], summary['reference_summary'],avg=True)","metadata":{"id":"zTCns0rhybbx","execution":{"iopub.status.busy":"2023-10-26T12:39:31.655746Z","iopub.execute_input":"2023-10-26T12:39:31.656091Z","iopub.status.idle":"2023-10-26T12:39:31.785650Z","shell.execute_reply.started":"2023-10-26T12:39:31.656057Z","shell.execute_reply":"2023-10-26T12:39:31.784750Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'rouge-1': {'r': 0.47198784141878747,\n  'p': 0.17856793892300263,\n  'f': 0.24567231305914564},\n 'rouge-2': {'r': 0.16000498494584542,\n  'p': 0.04726292079936201,\n  'f': 0.06881841492209151},\n 'rouge-l': {'r': 0.4195978716982551,\n  'p': 0.15330826439006093,\n  'f': 0.21332266858394297}}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}