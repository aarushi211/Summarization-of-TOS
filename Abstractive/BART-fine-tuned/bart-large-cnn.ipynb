{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5427149,"sourceType":"datasetVersion","datasetId":3140872}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e83d3a8a-ade4-4337-af3d-1034702286bd","cell_type":"code","source":"#!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html","metadata":{"id":"e83d3a8a-ade4-4337-af3d-1034702286bd","trusted":true},"outputs":[],"execution_count":null},{"id":"ecac4261-aea2-4af0-85ad-8bf4b81fbc84","cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install sentencepiece","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecac4261-aea2-4af0-85ad-8bf4b81fbc84","outputId":"2071b7d8-7a46-45e8-998c-a3b6535f6c62","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:19.197836Z","iopub.execute_input":"2025-07-22T07:38:19.198093Z","iopub.status.idle":"2025-07-22T07:38:25.440167Z","shell.execute_reply.started":"2025-07-22T07:38:19.198075Z","shell.execute_reply":"2025-07-22T07:38:25.439347Z"}},"outputs":[],"execution_count":1},{"id":"71b52894-ec51-4e6c-92e3-3df3bc21eb12","cell_type":"code","source":"import pandas as pd\n\nfilename = \"/kaggle/input/all-v1-transpose/all_v1_transpose.csv\"\n\ndf = pd.read_csv(filename)\ndf = df[['original_text','reference_summary']]\ndf.rename(columns = {'original_text':'source', 'reference_summary':'target'}, inplace = True)\nlen(df)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71b52894-ec51-4e6c-92e3-3df3bc21eb12","outputId":"92812a72-5c6f-4d40-cf08-59c013634d4b","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:28.097420Z","iopub.execute_input":"2025-07-22T07:38:28.097825Z","iopub.status.idle":"2025-07-22T07:38:28.463120Z","shell.execute_reply.started":"2025-07-22T07:38:28.097795Z","shell.execute_reply":"2025-07-22T07:38:28.462528Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"446"},"metadata":{}}],"execution_count":2},{"id":"b0416cc3-70b5-4b8e-933c-56d83f8890f5","cell_type":"code","source":"X = df['source']\ny = df['target']","metadata":{"id":"b0416cc3-70b5-4b8e-933c-56d83f8890f5","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:29.538073Z","iopub.execute_input":"2025-07-22T07:38:29.538338Z","iopub.status.idle":"2025-07-22T07:38:29.542647Z","shell.execute_reply.started":"2025-07-22T07:38:29.538319Z","shell.execute_reply":"2025-07-22T07:38:29.541819Z"}},"outputs":[],"execution_count":3},{"id":"a2d7ae11-5586-45cc-a152-b0833ec83717","cell_type":"code","source":"df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"a2d7ae11-5586-45cc-a152-b0833ec83717","outputId":"ed78c33c-de61-4da9-e9d9-5f42907ed6a6","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:31.533106Z","iopub.execute_input":"2025-07-22T07:38:31.533656Z","iopub.status.idle":"2025-07-22T07:38:31.542601Z","shell.execute_reply.started":"2025-07-22T07:38:31.533617Z","shell.execute_reply":"2025-07-22T07:38:31.541910Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                              source  \\\n0  welcome to the pokémon go video game services ...   \n1  by using our services you are agreeing to thes...   \n2  if you want to use certain features of the ser...   \n3  during game play please be aware of your surro...   \n4  subject to your compliance with these terms ni...   \n\n                                              target  \n0                                                hi.  \n1  by playing this game you agree to these terms....  \n2  you have to use google pokemon trainer club or...  \n3  don t die or hurt others and if you do it s no...  \n4  don t copy modify resell distribute or reverse...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>welcome to the pokémon go video game services ...</td>\n      <td>hi.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>by using our services you are agreeing to thes...</td>\n      <td>by playing this game you agree to these terms....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>if you want to use certain features of the ser...</td>\n      <td>you have to use google pokemon trainer club or...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>during game play please be aware of your surro...</td>\n      <td>don t die or hurt others and if you do it s no...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>subject to your compliance with these terms ni...</td>\n      <td>don t copy modify resell distribute or reverse...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"id":"dc76cc9a-0e6a-4a31-87b5-386c30416686","cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments,pipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM","metadata":{"id":"dc76cc9a-0e6a-4a31-87b5-386c30416686","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:31.960375Z","iopub.execute_input":"2025-07-22T07:38:31.961188Z","iopub.status.idle":"2025-07-22T07:38:40.642306Z","shell.execute_reply.started":"2025-07-22T07:38:31.961157Z","shell.execute_reply":"2025-07-22T07:38:40.641741Z"}},"outputs":[{"name":"stderr","text":"2025-07-22 07:38:37.347879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753169917.371684     167 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753169917.378950     167 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"id":"3b143b60-d6e5-4232-bdd6-1a2e640f2fe9","cell_type":"code","source":"class BartDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n        return item\n    def __len__(self):\n      return len(self.labels['input_ids'])","metadata":{"id":"3b143b60-d6e5-4232-bdd6-1a2e640f2fe9","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:40.643286Z","iopub.execute_input":"2025-07-22T07:38:40.643848Z","iopub.status.idle":"2025-07-22T07:38:40.648799Z","shell.execute_reply.started":"2025-07-22T07:38:40.643827Z","shell.execute_reply":"2025-07-22T07:38:40.648012Z"}},"outputs":[],"execution_count":6},{"id":"38188c63-d004-4b5c-af5f-9aa3ed841a4c","cell_type":"code","source":"def prepare_data(model_name, \n                 train_texts, train_labels, \n                 test_texts, test_labels):\n  \"\"\"\n  Prepare input data for model fine-tuning\n  \"\"\"\n  tokenizer = AutoTokenizer.from_pretrained(model_name)\n  prepare_test = False if test_texts is None or test_labels is None else True\n\n  def tokenize_data(texts, labels):\n    encodings = tokenizer(texts, truncation=True, padding=True, max_length = 600)\n    decodings = tokenizer(labels, truncation=True, padding=True, max_length = 256)\n    dataset_tokenized = BartDataset(encodings, decodings)\n    return dataset_tokenized\n\n  train_dataset = tokenize_data(train_texts, train_labels)\n  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n\n  return train_dataset, test_dataset, tokenizer","metadata":{"id":"38188c63-d004-4b5c-af5f-9aa3ed841a4c","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:40.649547Z","iopub.execute_input":"2025-07-22T07:38:40.649813Z","iopub.status.idle":"2025-07-22T07:38:40.684254Z","shell.execute_reply.started":"2025-07-22T07:38:40.649794Z","shell.execute_reply":"2025-07-22T07:38:40.683676Z"}},"outputs":[],"execution_count":7},{"id":"f87f2add-6cab-40e2-8cb6-6b57989df33f","cell_type":"code","source":"!pip install wandb -U","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:40.685469Z","iopub.execute_input":"2025-07-22T07:38:40.685701Z","iopub.status.idle":"2025-07-22T07:38:44.124246Z","shell.execute_reply.started":"2025-07-22T07:38:40.685672Z","shell.execute_reply":"2025-07-22T07:38:44.123483Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"}],"execution_count":8},{"id":"da6e031b-7847-492b-888f-c866569349ee","cell_type":"code","source":"import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:48.722673Z","iopub.execute_input":"2025-07-22T07:38:48.722973Z","iopub.status.idle":"2025-07-22T07:38:50.287117Z","shell.execute_reply.started":"2025-07-22T07:38:48.722947Z","shell.execute_reply":"2025-07-22T07:38:50.286374Z"}},"outputs":[],"execution_count":9},{"id":"0e88fbbb-46e1-4969-8aa3-ea25b122e7e8","cell_type":"code","source":"wandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:38:59.075494Z","iopub.execute_input":"2025-07-22T07:38:59.075810Z","iopub.status.idle":"2025-07-22T07:39:04.594713Z","shell.execute_reply.started":"2025-07-22T07:38:59.075788Z","shell.execute_reply":"2025-07-22T07:39:04.594128Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maarushi-jain211\u001b[0m (\u001b[33mfaltu-team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":11},{"id":"dd1fb01b-1808-4c3f-bcb1-04a38074e72b","cell_type":"code","source":"def prepare_fine_tuning(model_name, tokenizer, train_dataset, test_dataset, freeze_encoder=False, output_dir='./results'):\n  \"\"\"\n  Prepare configurations and base model for fine-tuning\n  \"\"\"\n  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n  model = BartForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n\n  if test_dataset is not None:\n    training_args = TrainingArguments(\n      output_dir=output_dir,           # output directory\n      num_train_epochs=5,              # total number of training epochs\n      per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n      per_device_eval_batch_size=1,    # batch size for evaluation, can increase if memory allows\n      save_steps=500,                  # number of updates steps before checkpoint saves\n      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n      eval_strategy='steps',     # evaluation strategy to adopt during training\n      eval_steps=100,                  # number of update steps before evaluation\n      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n      weight_decay=0.01,               # strength of weight decay\n      logging_dir='./logs',            # directory for storing logs\n      logging_steps=100,\n    )\n\n    trainer = Trainer(\n      model=model,                         # the instantiated 🤗 Transformers model to be trained\n      args=training_args,                  # training arguments, defined above\n      train_dataset=train_dataset,         # training dataset\n      eval_dataset=test_dataset,           # evaluation dataset\n      tokenizer=tokenizer\n    )\n\n  else:\n    training_args = TrainingArguments(\n      output_dir=output_dir,           # output directory\n      num_train_epochs=5,              # total number of training epochs\n      per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n      save_steps=500,                  # number of updates steps before checkpoint saves\n      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n      weight_decay=0.01,               # strength of weight decay\n      logging_dir='./logs',            # directory for storing logs\n      logging_steps=100,\n    )\n\n    trainer = Trainer(\n      model=model,                         # the instantiated 🤗 Transformers model to be trained\n      args=training_args,                  # training arguments, defined above\n      train_dataset=train_dataset,         # training dataset\n      tokenizer=tokenizer\n    )\n\n  return trainer","metadata":{"id":"dd1fb01b-1808-4c3f-bcb1-04a38074e72b","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:39:07.407375Z","iopub.execute_input":"2025-07-22T07:39:07.407939Z","iopub.status.idle":"2025-07-22T07:39:07.414887Z","shell.execute_reply.started":"2025-07-22T07:39:07.407915Z","shell.execute_reply":"2025-07-22T07:39:07.414131Z"}},"outputs":[],"execution_count":12},{"id":"92026c77-b10b-4198-8bb2-551191398961","cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","metadata":{"id":"92026c77-b10b-4198-8bb2-551191398961","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:39:11.721567Z","iopub.execute_input":"2025-07-22T07:39:11.722180Z","iopub.status.idle":"2025-07-22T07:39:11.732939Z","shell.execute_reply.started":"2025-07-22T07:39:11.722157Z","shell.execute_reply":"2025-07-22T07:39:11.732108Z"}},"outputs":[],"execution_count":13},{"id":"c7d3065e-926f-48fd-9ac5-98ae37df6694","cell_type":"code","source":"train_texts, train_labels = list(X_train), list(y_train)\ntest_texts, test_labels = list(X_test), list(y_test)","metadata":{"id":"c7d3065e-926f-48fd-9ac5-98ae37df6694","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:39:11.981309Z","iopub.execute_input":"2025-07-22T07:39:11.981569Z","iopub.status.idle":"2025-07-22T07:39:11.985536Z","shell.execute_reply.started":"2025-07-22T07:39:11.981552Z","shell.execute_reply":"2025-07-22T07:39:11.984838Z"}},"outputs":[],"execution_count":14},{"id":"6ac6a141-14e6-4196-aefb-862ee0ef8aa2","cell_type":"code","source":"model_name = 'facebook/bart-large-cnn'\ntrain_dataset,test_dataset, tokenizer = prepare_data(model_name, train_texts, train_labels,test_texts,test_labels)\ntrainer = prepare_fine_tuning(model_name, tokenizer, train_dataset,test_dataset)\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":715},"id":"6ac6a141-14e6-4196-aefb-862ee0ef8aa2","outputId":"674c85cc-2519-48fd-8aaa-4b40ae31a457","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:39:12.268195Z","iopub.execute_input":"2025-07-22T07:39:12.268441Z","iopub.status.idle":"2025-07-22T07:51:56.593241Z","shell.execute_reply.started":"2025-07-22T07:39:12.268423Z","shell.execute_reply":"2025-07-22T07:51:56.592378Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_167/3239069479.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250722_073914-whyezrck</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/faltu-team/huggingface/runs/whyezrck' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/faltu-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/faltu-team/huggingface' target=\"_blank\">https://wandb.ai/faltu-team/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/faltu-team/huggingface/runs/whyezrck' target=\"_blank\">https://wandb.ai/faltu-team/huggingface/runs/whyezrck</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='890' max='890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [890/890 12:31, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>5.349600</td>\n      <td>0.853842</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.788800</td>\n      <td>0.476132</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.538400</td>\n      <td>0.415667</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.422800</td>\n      <td>0.423913</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.311300</td>\n      <td>0.452233</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.290500</td>\n      <td>0.451973</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.219000</td>\n      <td>0.442567</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.110900</td>\n      <td>0.538207</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=890, training_loss=0.909958102194111, metrics={'train_runtime': 761.784, 'train_samples_per_second': 2.337, 'train_steps_per_second': 1.168, 'total_flos': 2260222377984000.0, 'train_loss': 0.909958102194111, 'epoch': 5.0})"},"metadata":{}}],"execution_count":15},{"id":"6a7db8cf-802d-45dc-b111-11c220048ec3","cell_type":"code","source":"trainer.evaluate(test_dataset)\nwandb.finish()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"6a7db8cf-802d-45dc-b111-11c220048ec3","outputId":"d42e3770-d83e-4cb8-e323-379a8af59b8e","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:55:55.735311Z","iopub.execute_input":"2025-07-22T07:55:55.735835Z","iopub.status.idle":"2025-07-22T07:56:06.263404Z","shell.execute_reply.started":"2025-07-22T07:55:55.735809Z","shell.execute_reply":"2025-07-22T07:56:06.262678Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [45/45 00:09]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▂▁▁▂▂▁▃▃</td></tr><tr><td>eval/runtime</td><td>▃▄▅▆█▆▂▄▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▅▄▃▁▃▇▅█</td></tr><tr><td>eval/steps_per_second</td><td>▆▅▄▃▁▃▇▅█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▃▆▂▁▃▂▃</td></tr><tr><td>train/learning_rate</td><td>▁▃▄▆█▆▄▁</td></tr><tr><td>train/loss</td><td>█▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.51449</td></tr><tr><td>eval/runtime</td><td>10.1001</td></tr><tr><td>eval/samples_per_second</td><td>8.911</td></tr><tr><td>eval/steps_per_second</td><td>4.455</td></tr><tr><td>total_flos</td><td>2260222377984000.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>890</td></tr><tr><td>train/grad_norm</td><td>6.15701</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1109</td></tr><tr><td>train_loss</td><td>0.90996</td></tr><tr><td>train_runtime</td><td>761.784</td></tr><tr><td>train_samples_per_second</td><td>2.337</td></tr><tr><td>train_steps_per_second</td><td>1.168</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">./results</strong> at: <a href='https://wandb.ai/faltu-team/huggingface/runs/whyezrck' target=\"_blank\">https://wandb.ai/faltu-team/huggingface/runs/whyezrck</a><br> View project at: <a href='https://wandb.ai/faltu-team/huggingface' target=\"_blank\">https://wandb.ai/faltu-team/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250722_073914-whyezrck/logs</code>"},"metadata":{}}],"execution_count":16},{"id":"6dd7030f-928c-490a-a931-dc7e84d9a64d","cell_type":"code","source":"import os\nif not os.path.exists('./ouput_model/'):\n    os.makedirs('./ouput_model/')\ntrainer.model.save_pretrained(\"./ouput_model/\")","metadata":{"id":"6dd7030f-928c-490a-a931-dc7e84d9a64d","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:56:06.264906Z","iopub.execute_input":"2025-07-22T07:56:06.265267Z","iopub.status.idle":"2025-07-22T07:56:09.163706Z","shell.execute_reply.started":"2025-07-22T07:56:06.265236Z","shell.execute_reply":"2025-07-22T07:56:09.163076Z"}},"outputs":[],"execution_count":17},{"id":"qqIpPmpjOufw","cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"id":"qqIpPmpjOufw","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T07:56:09.164656Z","iopub.execute_input":"2025-07-22T07:56:09.164944Z","iopub.status.idle":"2025-07-22T07:56:09.168666Z","shell.execute_reply.started":"2025-07-22T07:56:09.164915Z","shell.execute_reply":"2025-07-22T07:56:09.168000Z"}},"outputs":[],"execution_count":18},{"id":"9dc6d913-5eb2-406c-a240-0a4522e6b403","cell_type":"code","source":"from transformers import PretrainedConfig\nconfig = PretrainedConfig.from_json_file('./ouput_model/config.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:03:26.796622Z","iopub.execute_input":"2025-07-22T08:03:26.797280Z","iopub.status.idle":"2025-07-22T08:03:26.801549Z","shell.execute_reply.started":"2025-07-22T08:03:26.797257Z","shell.execute_reply":"2025-07-22T08:03:26.800814Z"}},"outputs":[],"execution_count":20},{"id":"522b802d-f7ff-4459-9249-31c1b2f201fc","cell_type":"code","source":"model = BartForConditionalGeneration.from_pretrained(\"./ouput_model/\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:04:04.976107Z","iopub.execute_input":"2025-07-22T08:04:04.976912Z","iopub.status.idle":"2025-07-22T08:04:05.905668Z","shell.execute_reply.started":"2025-07-22T08:04:04.976879Z","shell.execute_reply":"2025-07-22T08:04:05.904712Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":22},{"id":"c14d7b77-e4d7-4412-9e11-dabfd01b71b7","cell_type":"code","source":"def summarize(text):\n  input_tokenized = tokenizer.encode(text, return_tensors='pt',max_length=1024,truncation=True).to(device)\n  summary_ids = model.generate(input_tokenized,\n                                  num_beams=9,\n                                  no_repeat_ngram_size=3,\n                                  length_penalty=2.0,\n                                  min_length=50,\n                                  max_length=150,\n                                  early_stopping=True)\n  summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n\n  return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:04:07.992315Z","iopub.execute_input":"2025-07-22T08:04:07.993076Z","iopub.status.idle":"2025-07-22T08:04:07.998620Z","shell.execute_reply.started":"2025-07-22T08:04:07.993045Z","shell.execute_reply":"2025-07-22T08:04:07.997890Z"}},"outputs":[],"execution_count":23},{"id":"dc37b25d-b582-4613-8465-fd922d2c6c13","cell_type":"code","source":"y_pred = X_test.apply(lambda x: summarize(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:04:09.755314Z","iopub.execute_input":"2025-07-22T08:04:09.755911Z","iopub.status.idle":"2025-07-22T08:05:32.444721Z","shell.execute_reply.started":"2025-07-22T08:04:09.755884Z","shell.execute_reply":"2025-07-22T08:05:32.443909Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1730: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"}],"execution_count":24},{"id":"0efd0742-9c01-4df5-9b53-dd64bb09ad46","cell_type":"code","source":"summary = pd.concat([y_test.to_frame(name=\"reference_summary\"), y_pred.to_frame(name=\"generated_summary\")], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:05:34.891566Z","iopub.execute_input":"2025-07-22T08:05:34.892342Z","iopub.status.idle":"2025-07-22T08:05:34.898664Z","shell.execute_reply.started":"2025-07-22T08:05:34.892307Z","shell.execute_reply":"2025-07-22T08:05:34.897860Z"}},"outputs":[],"execution_count":25},{"id":"7c0e6a01-f7cd-4366-9658-b4e89876d601","cell_type":"code","source":"%%capture\n!pip install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:05:54.032231Z","iopub.execute_input":"2025-07-22T08:05:54.032515Z","iopub.status.idle":"2025-07-22T08:05:57.398011Z","shell.execute_reply.started":"2025-07-22T08:05:54.032494Z","shell.execute_reply":"2025-07-22T08:05:57.397187Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":27},{"id":"452f3a2a-342d-4e6f-a6d1-cc4ec504becf","cell_type":"code","source":"from rouge import Rouge\nrouge = Rouge()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:06:42.600241Z","iopub.execute_input":"2025-07-22T08:06:42.601045Z","iopub.status.idle":"2025-07-22T08:06:42.609792Z","shell.execute_reply.started":"2025-07-22T08:06:42.601013Z","shell.execute_reply":"2025-07-22T08:06:42.609300Z"}},"outputs":[],"execution_count":28},{"id":"4442257b-6d2d-4e45-bc60-be82524e0c70","cell_type":"code","source":"rouge.get_scores(summary['generated_summary'], summary['reference_summary'],avg=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:06:44.233238Z","iopub.execute_input":"2025-07-22T08:06:44.233501Z","iopub.status.idle":"2025-07-22T08:06:44.296363Z","shell.execute_reply.started":"2025-07-22T08:06:44.233482Z","shell.execute_reply":"2025-07-22T08:06:44.295823Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'rouge-1': {'r': 0.5487259156176463,\n  'p': 0.20253723733690462,\n  'f': 0.28280399819468766},\n 'rouge-2': {'r': 0.30034819915052285,\n  'p': 0.09504447630698805,\n  'f': 0.13729926939726517},\n 'rouge-l': {'r': 0.50774506222993,\n  'p': 0.18688956500971873,\n  'f': 0.26117554323676256}}"},"metadata":{}}],"execution_count":29},{"id":"69f21619-68bb-46c4-a7c3-23159cfe4bc6","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}