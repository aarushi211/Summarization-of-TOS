{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f708f345-9bbe-4aa1-93be-342301413026",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fc46d6-dbb8-4f5c-9063-3f107da49481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aarus\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "billsum_dataset = load_dataset(\"billsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980fbedf-991a-4f61-aa16-b51f9fa0c0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 18949\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "    ca_test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 1237\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0682f740-611f-4e00-a431-7cb167531b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = billsum_dataset['train']\n",
    "test = billsum_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e4c23-c255-48ee-b62a-6dd0af7dd32e",
   "metadata": {},
   "source": [
    "### Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48733cb-10a6-4082-b142-1c2a5e60f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_name)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9997146-8aae-44ee-80b3-4ba5aa803bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                            | 0/18949 [00:00<?, ? examples/s]C:\\Users\\aarus\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map:  58%|█████████████████████████████████████▏                          | 11000/18949 [05:18<03:50, 34.50 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTokenizing datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m tokenized_train_dataset = train_dataset.map(preprocess_function, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     26\u001b[39m tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTokenization complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset._map_single(**dataset_kwargs):\n\u001b[32m   3080\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m   3081\u001b[39m                 shards_done += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\datasets\\arrow_dataset.py:3525\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3524\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3525\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[32m   3526\u001b[39m         num_examples_in_batch = \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[32m   3527\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\datasets\\arrow_dataset.py:3475\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3474\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3475\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, apply_function(example, i, offset=offset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\datasets\\arrow_dataset.py:3398\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3397\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3398\u001b[39m processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mpreprocess_function\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m      5\u001b[39m model_inputs = tokenizer(\n\u001b[32m      6\u001b[39m     examples[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m      7\u001b[39m     max_length=MAX_INPUT_LENGTH, \n\u001b[32m      8\u001b[39m     padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m,  \n\u001b[32m      9\u001b[39m     truncation=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tokenizer.as_target_tokenizer():\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     labels = tokenizer(\n\u001b[32m     14\u001b[39m         examples[\u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     15\u001b[39m         max_length=MAX_OUTPUT_LENGTH, \n\u001b[32m     16\u001b[39m         padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     17\u001b[39m         truncation=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     20\u001b[39m model_inputs[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m] = labels[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2910\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2908\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2909\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2910\u001b[39m     encodings = \u001b[38;5;28mself\u001b[39m._call_one(text=text, text_pair=text_pair, **all_kwargs)\n\u001b[32m   2911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2912\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2998\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2993\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2994\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2995\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2996\u001b[39m         )\n\u001b[32m   2997\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m2998\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2999\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3000\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   3001\u001b[39m         padding=padding,\n\u001b[32m   3002\u001b[39m         truncation=truncation,\n\u001b[32m   3003\u001b[39m         max_length=max_length,\n\u001b[32m   3004\u001b[39m         stride=stride,\n\u001b[32m   3005\u001b[39m         is_split_into_words=is_split_into_words,\n\u001b[32m   3006\u001b[39m         pad_to_multiple_of=pad_to_multiple_of,\n\u001b[32m   3007\u001b[39m         padding_side=padding_side,\n\u001b[32m   3008\u001b[39m         return_tensors=return_tensors,\n\u001b[32m   3009\u001b[39m         return_token_type_ids=return_token_type_ids,\n\u001b[32m   3010\u001b[39m         return_attention_mask=return_attention_mask,\n\u001b[32m   3011\u001b[39m         return_overflowing_tokens=return_overflowing_tokens,\n\u001b[32m   3012\u001b[39m         return_special_tokens_mask=return_special_tokens_mask,\n\u001b[32m   3013\u001b[39m         return_offsets_mapping=return_offsets_mapping,\n\u001b[32m   3014\u001b[39m         return_length=return_length,\n\u001b[32m   3015\u001b[39m         verbose=verbose,\n\u001b[32m   3016\u001b[39m         split_special_tokens=split_special_tokens,\n\u001b[32m   3017\u001b[39m         **kwargs,\n\u001b[32m   3018\u001b[39m     )\n\u001b[32m   3019\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   3021\u001b[39m         text=text,\n\u001b[32m   3022\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3040\u001b[39m         **kwargs,\n\u001b[32m   3041\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3199\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3189\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3190\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3191\u001b[39m     padding=padding,\n\u001b[32m   3192\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3196\u001b[39m     **kwargs,\n\u001b[32m   3197\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._batch_encode_plus(\n\u001b[32m   3200\u001b[39m     batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3201\u001b[39m     add_special_tokens=add_special_tokens,\n\u001b[32m   3202\u001b[39m     padding_strategy=padding_strategy,\n\u001b[32m   3203\u001b[39m     truncation_strategy=truncation_strategy,\n\u001b[32m   3204\u001b[39m     max_length=max_length,\n\u001b[32m   3205\u001b[39m     stride=stride,\n\u001b[32m   3206\u001b[39m     is_split_into_words=is_split_into_words,\n\u001b[32m   3207\u001b[39m     pad_to_multiple_of=pad_to_multiple_of,\n\u001b[32m   3208\u001b[39m     padding_side=padding_side,\n\u001b[32m   3209\u001b[39m     return_tensors=return_tensors,\n\u001b[32m   3210\u001b[39m     return_token_type_ids=return_token_type_ids,\n\u001b[32m   3211\u001b[39m     return_attention_mask=return_attention_mask,\n\u001b[32m   3212\u001b[39m     return_overflowing_tokens=return_overflowing_tokens,\n\u001b[32m   3213\u001b[39m     return_special_tokens_mask=return_special_tokens_mask,\n\u001b[32m   3214\u001b[39m     return_offsets_mapping=return_offsets_mapping,\n\u001b[32m   3215\u001b[39m     return_length=return_length,\n\u001b[32m   3216\u001b[39m     verbose=verbose,\n\u001b[32m   3217\u001b[39m     split_special_tokens=split_special_tokens,\n\u001b[32m   3218\u001b[39m     **kwargs,\n\u001b[32m   3219\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\transformers\\tokenization_utils.py:887\u001b[39m, in \u001b[36mPreTrainedTokenizer._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    885\u001b[39m     ids, pair_ids = ids_or_pair_ids\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m first_ids = get_input_ids(ids)\n\u001b[32m    888\u001b[39m second_ids = get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    889\u001b[39m input_ids.append((first_ids, second_ids))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\transformers\\tokenization_utils.py:854\u001b[39m, in \u001b[36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m         tokens = \u001b[38;5;28mself\u001b[39m.tokenize(text, **kwargs)\n\u001b[32m    855\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\transformers\\tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28mself\u001b[39m._tokenize(token))\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\transformers\\models\\led\\tokenization_led.py:270\u001b[39m, in \u001b[36mLEDTokenizer._tokenize\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Tokenize a string.\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m bpe_tokens = []\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m    271\u001b[39m     token = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    272\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    273\u001b[39m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[32m    274\u001b[39m     bpe_tokens.extend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bpe(token).split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\indicf5\\Lib\\site-packages\\regex\\regex.py:338\u001b[39m, in \u001b[36mfindall\u001b[39m\u001b[34m(pattern, string, flags, pos, endpos, overlapped, concurrent, timeout, ignore_unused, **kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a list of all matches in the string. The matches may be overlapped\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mif overlapped is True. If one or more groups are present in the pattern,\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03mreturn a list of groups; this will be a list of tuples if the pattern has\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[33;03mmore than one group. Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[32m    337\u001b[39m pat = _compile(pattern, flags, ignore_unused, kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pat.findall(string, pos, endpos, overlapped, concurrent, timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "MAX_INPUT_LENGTH = 16384  \n",
    "MAX_OUTPUT_LENGTH = 512    \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['text'], \n",
    "        max_length=MAX_INPUT_LENGTH, \n",
    "        padding=\"max_length\",  \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['summary'], \n",
    "            max_length=MAX_OUTPUT_LENGTH, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9425fc-00b0-4d87-b351-5fda3ba734be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (indicf5)",
   "language": "python",
   "name": "indicf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
